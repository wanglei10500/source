---
title: hadoop
tags:
 - hadoop
categories: 经验分享
---

## 存储体系
### 存储体系概述
#### 块管理器BlockManager的实现
块管理器BlockManager是Spark存储体系中的核心组件 Driver Application和Executer都会创建BlockManager

BlockInfo:TimeStampedHashMap[BlockId,BlockInfo] 用于BlockManager缓存BlockId及对应的BlockInfo BlockManager主要由以下部分组成：
* shuffle客户端ShuffleClient
* BlockManagerMaster(对存在于所有Executor上的BlockManager统一管理)
* 磁盘块管理器DiskBlockManager
* 内存存储MemoryStore
* 磁盘存储DiskStore
* tachyon存储TachyonStore
* 非广播Block清理器metadataCleaner和广播Block清理器broadcastCleaner
* 压缩算法实现CompressionCodec

BlockManager要生效，必须要初始化，它的初始化步骤：
1. BlockTransferService的初始化和ShuffleClient的初始化 ShuffleClient默认是BlockTransferService 当有外部的ShuffleService时 调用外部ShuffleService的初始化方法
2. BlockManagerId和ShuffleServerId的创建。当有外部的ShuffleService时，创建新的BlockManagerId，否则ShuffleServerId默认使用当前BlockManager的BlockManagerId
3. 向BlockManagerMaster注册BlockManagerId
#### Spark存储体系架构
![spark](http://images2015.cnblogs.com/blog/739637/201707/739637-20170714172030775-1507870285.png)
1. 记号1表示Executor的BlockManager与Driver的BlockManager进行消息通信 例如：注册BlockManager、更新Block信息、获取Block所在的BlockManager、删除Executor等
2. 记号2表示BlockManager的读操作(例如get、doGetLocal以及BlockManager内部进行的MemoryStore、DiskStore、TachyonStore的getBytes、getValues等操作)和写操作(例如doPut、putSingle、putBytes以及BlockManager内部进行的MemoryStore、DiskStore的putBytes、putArray、putIterator等操作)
3. 记号3表示当MemoryStore的内存不足时，写入DiskStore，而DiskStore实现依赖于DiskBlockManager
4. 记号4表示通过访问远端节点的Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block
5. 记号5表示远端节点的Executor的BlockManager访问本地Executor的BlockManager中的TransportServer提供的RPC服务下载或上传Block
6. 记号6表示当存储体系选择TachyonStore作为存储时，对于BlockManager的读写操作实际调用了TranyonStore的putBytes、putArray、putIterator、getBytes、getValues等
### shuffle服务与客户端
为什么需要把由Netty实现的网络服务组件也放到存储体系里面？这是由于Spark是分布式部署的，每个Task最终都运行在不同的机器节点上。map任务的输出结果直接存储到map任务所在机器的存储体系中，reduce任务极有可能不在同一机器上运行，所以需要远程下载map任务的中间输出。因此将ShuffleClient放到存储体系是最合适的

ShuffleCLient并不像它的名字一样 是shuffle的客户端，它不光是将shuffle文件上传到其他Executer或者下载到本地的客户端，也提供了可以被其他Executor访问的shuffle服务 采用Netty作为shuffle server

当有外部的ShuffleClient时，新建ExternalShuffleClient，否则默认为BlockTransferService BlockTransferService只有在其init方法被调用，即被初始化后才提供服务。以默认的NettyBlockTransferService的init方法为例 NettyBlockTransferService初始化步骤：
1. 创建RpcServer
2. 构造TransportContext
3. 创建RPC客户端工厂TransportClientFactory
4. 创建Netty服务器TransportServer，可以修改属性spark.BlockManager.port(默认为0,表示随机选择)改变TransportServer的端口
#### Block的RPC服务
当map任务与reduce任务处于不同节点时，reduce任务需要从远端节点下载map任务的中间输出，因此NettyBlockRpcServer提供打开，即下载Block文件的功能 一些情况下，为了容错需要将Block的数据备份到其他节点上，所以NettyBlockRpcServer还提供了上传Block文件的RPC服务
#### 构造传输上下文TransportContext
TransportContext用于维护传输上下文 既可以创建Netty服务，也可以创建Netty访问客户端。TransportContext的组成如下：
* TransportConf:主要控制Netty框架提供的shuffle的I/O交互的客户端和服务端线程数量
* RpcHandle:负责shuffle的I/O服务端在接收到客户端的RPC请求后，提供打开Block或者上传Block的RPC处理，此处即为NettyBlockRpcServer
* decoder:在shuffle的I/O服务端对客户端传来的ByteBuf进行解析，防止丢包和解析错误
* encoder:在shuffle的I/O客户端对消息内容进行编码，防止服务端丢包和解析错误
为什么需要MessageEncoder和MessageDecoder？在基于流的传输里(TCP/IP) 接收到的数据首先会被存储到一个socket接收缓冲里。不幸的是，基于流的传输并不是一个数据包队列，而是一个字节队列，不能保证准确处理，防止丢包
#### RPC客户端工厂TransportClientFactory
TransportClientFactory是创建Netty客户端TransportClient的工厂类，TransportClient用于向Netty服务端发送RPC请求。TransportContext的createClientFactory方法用于创建TransportClientFactory

TransportClientFactory由以下部分组成：
* clientBootstraps:用于缓存客户端列表
* connectionPool:用于缓存客户端连接
* numConnectionsPerPeer:节点之间取数据的连接数，可以使用spark.shuffle.io.numConnectionsPerPeer来配置，默认为1
* socketChannelClass:客户端channel被创建时使用的类，可以使用属性spark.shuffle.io.mode来配置，默认为NioSocketChannel
* workGroup:根据Netty规范，客户端只有work组，所以此处创建workGroup，实际是NioEventLoopGroup
* pooledAllocator:汇集ByteBuf但对本地线程缓存禁用的分配器

NIO：Java中New IO简称，特点：
* 为所有的原始类型提供(Buffer)缓存支持
* 字符集编码解码解决方案
* 提供一个新的原始I/O抽象Channel，支持锁和内存映射文件的文件访问接口
* 提供多路非阻塞式(non-bloking)的高伸缩性网络I/O
#### Netty服务器TransportServer
TransportServer提供了Netty实现的服务端，用于提供RPC服务(比如上传、下载)

对TransportServer初始化,通过使用Netty框架EventLoopGroup、ServerBootstrap等API创建shuffle的I/O交互的服务端

ServerBootstrap的childHandler方法调用了TransportContext的initializePipeline initializePipeline中创建了TransportChannelHandler并将它绑定到SocketChannel的pipeline的handle中
#### 获取远程shuffle文件
NettyBlockTransferService的fetchBlocks方法用于获取远程shuffle文件 实际使用NettyBlockTransferService中创建的Netty服务
#### 上传shuffle文件
NettyBlockTransferService的uploadBlock方法用于上传shuffle文件到远程Executor文件到远程Executor，实际也是利用NettyBlockTransferService中创建的Netty服务 NettyBlockTransferService上传Block的步骤如下:
1. 创建Netty服务的客户端，客户端连接的hostname和port是随机选择的BlockManager的hostname和port
2. 将Block的存储级别StorageLevel序列化
3. 将Block的ByteBuffer转化为数组,便于序列化
4. 将appId、execId、blockId、序列化的StorageLevel、转换为数组的Block封装为uploadBlock，并将UploadBlock序列化为字节数组
5. 最终调用Netty客户端的sendRpc方法将字节数组上传，回调函数RpcResponseCallback根据RPC的结果更改上传状态
### BlockManagerMaster对BlockManager的管理
Driver上的BlockManagerMaster对存在于Executor上的BlockManager统一管理，比如Executor需要向Driver发送注册BlockManager、更新Executor上Block的最新信息、询问所需要Block目前所在的位置以及当Executor运行结束需要将此Executor移除等

Driver与Executor在不同机器，如何实现？Driver上的BlockManagerMaster会持有BlockManagerMasterActor,所有Executor也会从ActorSystem中获取BlockManagerMasterActor的引用，所有Executor与Driver的交互都依赖它
#### BlockManagerMasterActor
只存在与Driver上。Executor从ActorSystem获取BlockManagerMasterActor的引用，然后给BlockManagerMasterActor发送消息，实现和Driver交互

BlockManagerMasterActor维护了很多缓存数据结构：
* blockManagerInfo：缓存所有的BlockManagerId及其BlockManager的信息
* blockManagerIdByExecutor:缓存executorId与其拥有的BlockManagerId之间的映射关系
* blockLocations：缓存Block与BlockManagerId的映射关系
#### 询问Driver并获取回复方法
在Executor中，所有与Driver上BlockManagerMaster的交互方法最终都调用了askDriverWithReply

askDriverWithReply调用了AkkaUtils.askWithReply方法，askWithReply方法实际使用ActorSystem向BlockManagerMasterActor发送任何信息，每条消息的最大尝试次数是3次，间隔3000毫秒，请求超时时间30秒
#### 向BlockManagerMaster注册BlockManagerId
Executor或者Driver自身的BlockManager在初始化时，需要向Driver的BlockManager注册BlockManager信息

消息内容包括BlockManagerId、最大内存、BlockManagerSlaveActor 消息体带有BlockManagerSlaveActor是为了便于接受BlockManagerMasterActor回复的消息 这些信息被封装为RegisterBlockManager

RegisterBlockManager消息会被BlockManagerMasterActor匹配并执行register方法注册BlockManager，并在register方法执行结束后向发送者BlockManagerSlaveActor发送一个简单的消息true

register方法确保blockManagerInfo持有消息中的blockManagerId及对应信息，并且确保每个Executor最多只能有一个blockManagerId，旧的blockManagerId会被移除，最后向listenerBus中post(推送)一个SparkListenerBlockManagerAdded事件
### 磁盘块管理器DiskBlockManager
#### DiskBlockManager的构造过程
BlockManager初始化时会创建DiskBlockManager，DiskBlockManager的构造步骤如下：
1. 调用createLocalDirs方法创建本地文件目录,然后创建二维数组subDirs，用来缓存一级目录localDirs及二级目录，其中二级目录的数量根据配置spark.diskStore.subDirectories获取，默认为64
2. 添加运行时环境结束的钩子，用于在进程关闭时创建线程，通过调用DiskBlockManager的stop方法，清除一些临时目录

DiskBlockManager为什么要创建二级目录结构？ 这是因为二级目录用于对文件进行散列存储，散列存储可以使所有文件都随机存放，写入或删除文件更方便，存取速度快，节省空间
#### 获取磁盘文件方法getFile
getFile处理步骤：
1. 根据文件名计算哈希值
2. 根据哈希值与本地文件一级目录的总数求余数，记为dirId
3. 根据哈希值与本地文件一级目录的总数求商数，此商数与二级目录的数目再求余数，记为subDirId
4. 如果dirId/subDirId目录存在，则获取dirId/subDirId目录下的文件，否则新建dirId/subDirId目录
#### 创建临时Block方法createTempShuffleBlock
当ShuffleMapTask运行结束需要把中间结果临时保存，此时就调用createTempShuffleBlock方法创建临时Block，并返回TempShuffleBlockId与其文件的对偶
### 磁盘存储DiskStore
当MemoryStore没有足够空间时，就会使用DiskStore将块存入磁盘。DiskStore继承自BlockStore,并实现了getBytes、putBytes、putArray、putIterator等方法
#### NIO读取方法getBytes
getBytes方法通过DiskBlockManager的getFile方法获取文件。然后使用NIO将文件读取到ByteBuffer
#### NIO写入方法putBytes
putBytes方法的作用是通过DiskBlockManager的getFile方法获取文件，然后使用NIO的Channel将ByteBuffer写入文件
#### 数组写入方法putArray
内部实际调用了putIterator
#### Iterator写入方法putIterator
putIterator其处理步骤：
1. 使用了DiskBlockManager的getFile获取blockId对应的Block文件，并封装为FileOutputStream
2. 调用BlockManager的dataSerializeStream方法，将FileOutputStream序列化并压缩。
3. 如果需要返回写入的数据(即returnValues等于true)则将写入的文件使用getBytes读取为ByteBuffer，与文件的长度一并封装到PutResult中并返回，否则只返回文件长度
### 内存存储MemoryStore
MemoryStore负责将没有序列化的Java对象数组或者序列化的ByteBuffer存储到内存中。整个MemoryStore的存储分为两块:

一块是被很多MemoryEntry占据的内存currentMemory，这些MemoryEntry实际是通过entries持有的 另一块是unrollMemoryMap通过占座方式占用的内存currentUnrollMemory

占座方式防止在向内存真正写入数据时，内存不足发生溢出，每个线程实际占用的空间实际是vector(SizeTrackingVector)占用的大小，但是unrollMemoryMap的大小会稍大些

* maxUnrollMemory：当前Driver或者Executor最多展开的Block所占用的内存，可以修改属性spark.storage.unrollFraction改变大小
* maxMemory:当前Driver或者Executor的最大内容
* currentMemory：当前Driver或者Executor已经使用的内存
* freeMemory：当前Driver或者Executor未使用的内存，freeMemory=maxMemory-currentMemory
* currentUnrollMemory：unrollMemoryMap中所有展开的Block的内存之和，即当前Driver或者Executor中所有线程展开的Block的内存之和
* unrollMemoryMap：当前Driver或者Executor中所有线程展开的Block都存入此Map中，key为线程ID，value为线程展开的所有块的内存大小总和

MemoryStore继承自BlockStore
#### 数据存储方法putBytes
如果Block可以被反序列化(即存储级别StorageLevel.Deserialized等于true)那么先对Block序列化，然后调用putIterator，否则调用tryToPut
#### Iterator写入方法putIterator详解
调用unrollSafely将块在内存中安全展开，如果返回数据的类型匹配Left(arrayValues)，则说明内存足够并调用putArray方法写入内存  如果返回数据的类型匹配Right(iteratorValues)则说明内存不足并写入磁盘或放弃
#### 安全展开方法unrollSafely
为了防止写入内存的数据过大，导致内存溢出，Spark采用了一种优化方案，在正式写入内存之前，先用逻辑方式申请内存，如果申请成功，再写入内存，这个过程成为安全展开 内存展开的步骤
1. 申请memoryThreshold的初始大小为initialMemoryThreshold
2. 如果Iterator[Any]中有元素并且keepUnrolling为true。则向vector中添加Iterator[Any]中的对象，elementsUnrolled自增1，如果Iterator[Any]中没有元素或者keepUnrolling不等于true 跳转到4
3. 如果elementsUnrolled%memoryCheckPeriod=0 则开始检查currentSize是否已经比memoryThreshold大 假如currentSize已经超过了memoryThreshold则需要再申请内存，申请内存大小amountToRequest=currentSize*memoryGrowthFactor-memoryThreshold
    如果申请失败，但是maxUnrollMemory>currentUnrollMemory，则要求释放当前Driver或者Executor的其他内存  释放内存必然伴随着其他Block被移入硬盘或者彻底清除，这些块的状态会在释放后返回。此时再次申请内存，memoryThreshold增加的大小为amountToRequest
4. 根据是否将Block完整地放入内存，以数组或者迭代器形式返回vector的数据
5. 最后在finally语句块里，还会计算本次展开块实际占用的空间amountToRelease,并更新unrollMemoryMap中当前线程占用的内存大小小于等于0，则从unrollMemoryMap中完全清除此线程的数据
#### 确认空闲内存方法ensureFreeSpace
ensureFreeSpace方法用于确认是否有足够内存，如果不足，会释放被MemoryEntry占用的内存

ensureFreeSpace的处理步骤：
1. space不能超过maxMemory的限制，否则返回。
2. 如果actualFreeMemory大小等于space，说明此时已经有充足的内存，不需要释放内存空间，直接返回。 如果actualFreeMemory小于space，则说明空闲空间不足，需要释放一部分已经占用的内存
3. 当actualFreeMemory+selectedMemory<space，则迭代entries获得blockId和MemoryEntry 如果blockIdToAdd的rddId是空或者blockIdToAdd的rddId不等于MemoryEntry对应blockId的rddId，则将blockId加入selectedBlocks
      selectedMemory增加MemoryEntry的大小
4. 当actualFreeMemory+selectedMemory>=space时，则说明可以腾出足够的内存空间。此时将selectedBlocks中所有的blockId对应于entries里的MemoryEntry取出，通过判断MemoryEntry是否可以反序列化
        分别转换为Array[Any]或者ByteBuffer，调用BlockManager的dropFromMemory方法，从内存中移除blockId及MemoryEntry，此方法返回移除的Block的状态
#### 内存写入方法putArray
        
