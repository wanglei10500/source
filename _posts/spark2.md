---
title: hadoop
tags:
 - hadoop
categories: 经验分享
---

## 存储体系
### 存储体系概述
#### 块管理器BlockManager的实现
块管理器BlockManager是Spark存储体系中的核心组件 Driver Application和Executer都会创建BlockManager

hdfs中的block是存储的最小单元，spark中的block是rdd在被task执行之前，其基本组成partition被BlockManager映射而来的一种抽象

BlockInfo:TimeStampedHashMap[BlockId,BlockInfo] 用于BlockManager缓存BlockId及对应的BlockInfo BlockManager主要由以下部分组成：
* shuffle客户端ShuffleClient
* BlockManagerMaster(对存在于所有Executor上的BlockManager统一管理)
* 磁盘块管理器DiskBlockManager
* 内存存储MemoryStore
* 磁盘存储DiskStore
* tachyon存储TachyonStore
* 非广播Block清理器metadataCleaner和广播Block清理器broadcastCleaner
* 压缩算法实现CompressionCodec

BlockManager要生效，必须要初始化，它的初始化步骤：
1. BlockTransferService的初始化和ShuffleClient的初始化 ShuffleClient默认是BlockTransferService 当有外部的ShuffleService时 调用外部ShuffleService的初始化方法
2. BlockManagerId和ShuffleServerId的创建。当有外部的ShuffleService时，创建新的BlockManagerId，否则ShuffleServerId默认使用当前BlockManager的BlockManagerId
3. 向BlockManagerMaster注册BlockManagerId
#### Spark存储体系架构
![spark](http://images2015.cnblogs.com/blog/739637/201707/739637-20170714172030775-1507870285.png)
1. 记号1表示Executor的BlockManager与Driver的BlockManager进行消息通信 例如：注册BlockManager、更新Block信息、获取Block所在的BlockManager、删除Executor等
2. 记号2表示BlockManager的读操作(例如get、doGetLocal以及BlockManager内部进行的MemoryStore、DiskStore、TachyonStore的getBytes、getValues等操作)和写操作(例如doPut、putSingle、putBytes以及BlockManager内部进行的MemoryStore、DiskStore的putBytes、putArray、putIterator等操作)
3. 记号3表示当MemoryStore的内存不足时，写入DiskStore，而DiskStore实现依赖于DiskBlockManager
4. 记号4表示通过访问远端节点的Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block
5. 记号5表示远端节点的Executor的BlockManager访问本地Executor的BlockManager中的TransportServer提供的RPC服务下载或上传Block
6. 记号6表示当存储体系选择TachyonStore作为存储时，对于BlockManager的读写操作实际调用了TranyonStore的putBytes、putArray、putIterator、getBytes、getValues等
### shuffle服务与客户端
为什么需要把由Netty实现的网络服务组件也放到存储体系里面？这是由于Spark是分布式部署的，每个Task最终都运行在不同的机器节点上。map任务的输出结果直接存储到map任务所在机器的存储体系中，reduce任务极有可能不在同一机器上运行，所以需要远程下载map任务的中间输出。因此将ShuffleClient放到存储体系是最合适的

ShuffleCLient并不像它的名字一样 是shuffle的客户端，它不光是将shuffle文件上传到其他Executer或者下载到本地的客户端，也提供了可以被其他Executor访问的shuffle服务 采用Netty作为shuffle server

当有外部的ShuffleClient时，新建ExternalShuffleClient，否则默认为BlockTransferService BlockTransferService只有在其init方法被调用，即被初始化后才提供服务。以默认的NettyBlockTransferService的init方法为例 NettyBlockTransferService初始化步骤：
1. 创建RpcServer
2. 构造TransportContext
3. 创建RPC客户端工厂TransportClientFactory
4. 创建Netty服务器TransportServer，可以修改属性spark.BlockManager.port(默认为0,表示随机选择)改变TransportServer的端口
#### Block的RPC服务
当map任务与reduce任务处于不同节点时，reduce任务需要从远端节点下载map任务的中间输出，因此NettyBlockRpcServer提供打开，即下载Block文件的功能 一些情况下，为了容错需要将Block的数据备份到其他节点上，所以NettyBlockRpcServer还提供了上传Block文件的RPC服务
#### 构造传输上下文TransportContext
TransportContext用于维护传输上下文 既可以创建Netty服务，也可以创建Netty访问客户端。TransportContext的组成如下：
* TransportConf:主要控制Netty框架提供的shuffle的I/O交互的客户端和服务端线程数量
* RpcHandle:负责shuffle的I/O服务端在接收到客户端的RPC请求后，提供打开Block或者上传Block的RPC处理，此处即为NettyBlockRpcServer
* decoder:在shuffle的I/O服务端对客户端传来的ByteBuf进行解析，防止丢包和解析错误
* encoder:在shuffle的I/O客户端对消息内容进行编码，防止服务端丢包和解析错误
为什么需要MessageEncoder和MessageDecoder？在基于流的传输里(TCP/IP) 接收到的数据首先会被存储到一个socket接收缓冲里。不幸的是，基于流的传输并不是一个数据包队列，而是一个字节队列，不能保证准确处理，防止丢包
#### RPC客户端工厂TransportClientFactory
TransportClientFactory是创建Netty客户端TransportClient的工厂类，TransportClient用于向Netty服务端发送RPC请求。TransportContext的createClientFactory方法用于创建TransportClientFactory

TransportClientFactory由以下部分组成：
* clientBootstraps:用于缓存客户端列表
* connectionPool:用于缓存客户端连接
* numConnectionsPerPeer:节点之间取数据的连接数，可以使用spark.shuffle.io.numConnectionsPerPeer来配置，默认为1
* socketChannelClass:客户端channel被创建时使用的类，可以使用属性spark.shuffle.io.mode来配置，默认为NioSocketChannel
* workGroup:根据Netty规范，客户端只有work组，所以此处创建workGroup，实际是NioEventLoopGroup
* pooledAllocator:汇集ByteBuf但对本地线程缓存禁用的分配器

NIO：Java中New IO简称，特点：
* 为所有的原始类型提供(Buffer)缓存支持
* 字符集编码解码解决方案
* 提供一个新的原始I/O抽象Channel，支持锁和内存映射文件的文件访问接口
* 提供多路非阻塞式(non-bloking)的高伸缩性网络I/O
#### Netty服务器TransportServer
TransportServer提供了Netty实现的服务端，用于提供RPC服务(比如上传、下载)

对TransportServer初始化,通过使用Netty框架EventLoopGroup、ServerBootstrap等API创建shuffle的I/O交互的服务端

ServerBootstrap的childHandler方法调用了TransportContext的initializePipeline initializePipeline中创建了TransportChannelHandler并将它绑定到SocketChannel的pipeline的handle中
#### 获取远程shuffle文件
NettyBlockTransferService的fetchBlocks方法用于获取远程shuffle文件 实际使用NettyBlockTransferService中创建的Netty服务
#### 上传shuffle文件
NettyBlockTransferService的uploadBlock方法用于上传shuffle文件到远程Executor文件到远程Executor，实际也是利用NettyBlockTransferService中创建的Netty服务 NettyBlockTransferService上传Block的步骤如下:
1. 创建Netty服务的客户端，客户端连接的hostname和port是随机选择的BlockManager的hostname和port
2. 将Block的存储级别StorageLevel序列化
3. 将Block的ByteBuffer转化为数组,便于序列化
4. 将appId、execId、blockId、序列化的StorageLevel、转换为数组的Block封装为uploadBlock，并将UploadBlock序列化为字节数组
5. 最终调用Netty客户端的sendRpc方法将字节数组上传，回调函数RpcResponseCallback根据RPC的结果更改上传状态
### BlockManagerMaster对BlockManager的管理
Driver上的BlockManagerMaster对存在于Executor上的BlockManager统一管理，比如Executor需要向Driver发送注册BlockManager、更新Executor上Block的最新信息、询问所需要Block目前所在的位置以及当Executor运行结束需要将此Executor移除等

Driver与Executor在不同机器，如何实现？Driver上的BlockManagerMaster会持有BlockManagerMasterActor,所有Executor也会从ActorSystem中获取BlockManagerMasterActor的引用，所有Executor与Driver的交互都依赖它
#### BlockManagerMasterActor
只存在与Driver上。Executor从ActorSystem获取BlockManagerMasterActor的引用，然后给BlockManagerMasterActor发送消息，实现和Driver交互

BlockManagerMasterActor维护了很多缓存数据结构：
* blockManagerInfo：缓存所有的BlockManagerId及其BlockManager的信息
* blockManagerIdByExecutor:缓存executorId与其拥有的BlockManagerId之间的映射关系
* blockLocations：缓存Block与BlockManagerId的映射关系
#### 询问Driver并获取回复方法
在Executor中，所有与Driver上BlockManagerMaster的交互方法最终都调用了askDriverWithReply

askDriverWithReply调用了AkkaUtils.askWithReply方法，askWithReply方法实际使用ActorSystem向BlockManagerMasterActor发送任何信息，每条消息的最大尝试次数是3次，间隔3000毫秒，请求超时时间30秒
#### 向BlockManagerMaster注册BlockManagerId
Executor或者Driver自身的BlockManager在初始化时，需要向Driver的BlockManager注册BlockManager信息

消息内容包括BlockManagerId、最大内存、BlockManagerSlaveActor 消息体带有BlockManagerSlaveActor是为了便于接受BlockManagerMasterActor回复的消息 这些信息被封装为RegisterBlockManager

RegisterBlockManager消息会被BlockManagerMasterActor匹配并执行register方法注册BlockManager，并在register方法执行结束后向发送者BlockManagerSlaveActor发送一个简单的消息true

register方法确保blockManagerInfo持有消息中的blockManagerId及对应信息，并且确保每个Executor最多只能有一个blockManagerId，旧的blockManagerId会被移除，最后向listenerBus中post(推送)一个SparkListenerBlockManagerAdded事件
### 磁盘块管理器DiskBlockManager
#### DiskBlockManager的构造过程
BlockManager初始化时会创建DiskBlockManager，DiskBlockManager的构造步骤如下：
1. 调用createLocalDirs方法创建本地文件目录,然后创建二维数组subDirs，用来缓存一级目录localDirs及二级目录，其中二级目录的数量根据配置spark.diskStore.subDirectories获取，默认为64
2. 添加运行时环境结束的钩子，用于在进程关闭时创建线程，通过调用DiskBlockManager的stop方法，清除一些临时目录

DiskBlockManager为什么要创建二级目录结构？ 这是因为二级目录用于对文件进行散列存储，散列存储可以使所有文件都随机存放，写入或删除文件更方便，存取速度快，节省空间
#### 获取磁盘文件方法getFile
getFile处理步骤：
1. 根据文件名计算哈希值
2. 根据哈希值与本地文件一级目录的总数求余数，记为dirId
3. 根据哈希值与本地文件一级目录的总数求商数，此商数与二级目录的数目再求余数，记为subDirId
4. 如果dirId/subDirId目录存在，则获取dirId/subDirId目录下的文件，否则新建dirId/subDirId目录
#### 创建临时Block方法createTempShuffleBlock
当ShuffleMapTask运行结束需要把中间结果临时保存，此时就调用createTempShuffleBlock方法创建临时Block，并返回TempShuffleBlockId与其文件的对偶
### 磁盘存储DiskStore
当MemoryStore没有足够空间时，就会使用DiskStore将块存入磁盘。DiskStore继承自BlockStore,并实现了getBytes、putBytes、putArray、putIterator等方法
#### NIO读取方法getBytes
getBytes方法通过DiskBlockManager的getFile方法获取文件。然后使用NIO将文件读取到ByteBuffer
#### NIO写入方法putBytes
putBytes方法的作用是通过DiskBlockManager的getFile方法获取文件，然后使用NIO的Channel将ByteBuffer写入文件
#### 数组写入方法putArray
内部实际调用了putIterator
#### Iterator写入方法putIterator
putIterator其处理步骤：
1. 使用了DiskBlockManager的getFile获取blockId对应的Block文件，并封装为FileOutputStream
2. 调用BlockManager的dataSerializeStream方法，将FileOutputStream序列化并压缩。
3. 如果需要返回写入的数据(即returnValues等于true)则将写入的文件使用getBytes读取为ByteBuffer，与文件的长度一并封装到PutResult中并返回，否则只返回文件长度
### 内存存储MemoryStore
MemoryStore负责将没有序列化的Java对象数组或者序列化的ByteBuffer存储到内存中。整个MemoryStore的存储分为两块:

一块是被很多MemoryEntry占据的内存currentMemory，这些MemoryEntry实际是通过entries持有的 另一块是unrollMemoryMap通过占座方式占用的内存currentUnrollMemory

占座方式防止在向内存真正写入数据时，内存不足发生溢出，每个线程实际占用的空间实际是vector(SizeTrackingVector)占用的大小，但是unrollMemoryMap的大小会稍大些

* maxUnrollMemory：当前Driver或者Executor最多展开的Block所占用的内存，可以修改属性spark.storage.unrollFraction改变大小
* maxMemory:当前Driver或者Executor的最大内容
* currentMemory：当前Driver或者Executor已经使用的内存
* freeMemory：当前Driver或者Executor未使用的内存，freeMemory=maxMemory-currentMemory
* currentUnrollMemory：unrollMemoryMap中所有展开的Block的内存之和，即当前Driver或者Executor中所有线程展开的Block的内存之和
* unrollMemoryMap：当前Driver或者Executor中所有线程展开的Block都存入此Map中，key为线程ID，value为线程展开的所有块的内存大小总和

MemoryStore继承自BlockStore
#### 数据存储方法putBytes
如果Block可以被反序列化(即存储级别StorageLevel.Deserialized等于true)那么先对Block序列化，然后调用putIterator，否则调用tryToPut
#### Iterator写入方法putIterator详解
调用unrollSafely将块在内存中安全展开，如果返回数据的类型匹配Left(arrayValues)，则说明内存足够并调用putArray方法写入内存  如果返回数据的类型匹配Right(iteratorValues)则说明内存不足并写入磁盘或放弃
#### 安全展开方法unrollSafely
为了防止写入内存的数据过大，导致内存溢出，Spark采用了一种优化方案，在正式写入内存之前，先用逻辑方式申请内存，如果申请成功，再写入内存，这个过程成为安全展开 内存展开的步骤
1. 申请memoryThreshold的初始大小为initialMemoryThreshold
2. 如果Iterator[Any]中有元素并且keepUnrolling为true。则向vector中添加Iterator[Any]中的对象，elementsUnrolled自增1，如果Iterator[Any]中没有元素或者keepUnrolling不等于true 跳转到4
3. 如果elementsUnrolled%memoryCheckPeriod=0 则开始检查currentSize是否已经比memoryThreshold大 假如currentSize已经超过了memoryThreshold则需要再申请内存，申请内存大小amountToRequest=currentSize*memoryGrowthFactor-memoryThreshold
    如果申请失败，但是maxUnrollMemory>currentUnrollMemory，则要求释放当前Driver或者Executor的其他内存  释放内存必然伴随着其他Block被移入硬盘或者彻底清除，这些块的状态会在释放后返回。此时再次申请内存，memoryThreshold增加的大小为amountToRequest
4. 根据是否将Block完整地放入内存，以数组或者迭代器形式返回vector的数据
5. 最后在finally语句块里，还会计算本次展开块实际占用的空间amountToRelease,并更新unrollMemoryMap中当前线程占用的内存大小小于等于0，则从unrollMemoryMap中完全清除此线程的数据
#### 确认空闲内存方法ensureFreeSpace
ensureFreeSpace方法用于确认是否有足够内存，如果不足，会释放被MemoryEntry占用的内存

ensureFreeSpace的处理步骤：
1. space不能超过maxMemory的限制，否则返回。
2. 如果actualFreeMemory大小等于space，说明此时已经有充足的内存，不需要释放内存空间，直接返回。 如果actualFreeMemory小于space，则说明空闲空间不足，需要释放一部分已经占用的内存
3. 当actualFreeMemory+selectedMemory<space，则迭代entries获得blockId和MemoryEntry 如果blockIdToAdd的rddId是空或者blockIdToAdd的rddId不等于MemoryEntry对应blockId的rddId，则将blockId加入selectedBlocks
      selectedMemory增加MemoryEntry的大小
4. 当actualFreeMemory+selectedMemory>=space时，则说明可以腾出足够的内存空间。此时将selectedBlocks中所有的blockId对应于entries里的MemoryEntry取出，通过判断MemoryEntry是否可以反序列化
        分别转换为Array[Any]或者ByteBuffer，调用BlockManager的dropFromMemory方法，从内存中移除blockId及MemoryEntry，此方法返回移除的Block的状态
#### 内存写入方法putArray
内存写入方法putArray首先对对象大小进行估算，然后写入内存。如果unrollSafely返回的数据匹配Left，整个Block是可以一次性放入内存的 ，此时调用putArray
#### 尝试写入内存方法tryToPut        
当Block不支持序列化时，会调用tryPut方法 MemoryStore的putArray方法时，也最终使用此方法

tryToPut也调用了ensureFreeSpace方法，之前已经在unrollSafely方法中调用过ensureFreeSpace了，因为在展开阶段，即便内存充足，当真正写入数据时依然可能内存不足，所以需要再次确认空间内存是否充足

如果内存充足或者迁移其他内存Block后有足够内存，则会创建MemoryEntry对象，并将此对象与其blockId放入entries中，currentMemory会上浮估算的大小size。如果此时内存不足，还要把此blockId对应的MemoryEntry对象迁移到磁盘或者清楚
#### 获取内存数据方法getBytes
getBytes方法用于从entries中获取MemoryEntry 如果MemoryEntry支持反序列化，则将MemoryEntry的value反序列化后返回，否则对MemoryEntry的value复制后返回
#### 获取数据方法getValues
getValues也用于从内存中获取数据，即从entries中获取MemoryEntry，并将blockId和value返回

### Tachyon存储TachyonStore
略
### 块管理器BlockManager
BlockManager自身的实现
#### 移出内存方法dropFromMemory
当内存不足时，可能需要腾出部分内存空间 步骤：
1. 从blockInfo:TimestampHashMap[BlockId,BlockInfo]中检查是否存在要迁移的blockId。如果存在，从BlockInfo中获取Block的StorageLevel
2. 如果StorageLevel允许存入磁盘，并且DiskStore中不存在此文件，那么调用DiskStore的putArray或者putBytes方法，将此Block存入磁盘
3. 从MemoryStore中清除此BlockId对应的Block
4. 使用getCurrentBlockStatus方法获取Block最新状态。如果此Block的tellMaster的属性为true，则调用reportBlockStatus方法给BlockManagerMasterActor报告状态
5. 从blockInfo中清除BlockId,并返回BlockId的状态
#### 状态报告方法reportBlockStatus
reportBlockStatus用于向BlockManagerMasterActor报告Block的状态并且重新注册BlockManager。 处理步骤：
1. 调用tryToReportBlockStatus方法，tryToReportBlockStatus调用了BlockManagerMaster的updateBlockInfo方法向BlockManagerMasterActor发送updateBlockInfo消息更新Block占用的内存大小、磁盘大小、存储级别等信息
2. 如果BlockManager还没有向BlockManagerMasterActor注册，则调用asyncReregister方法，reregister实际调用了BlockManagerMaster的RegisterBlockManager方法和reportAllBlocks方法，reportAllBlocks方法实际也是调用了tryToReportBlockStatus
#### 单对象块写入方法
putSingle方法用于将由一个对象构成的Block写入存储系统。putSingle经过层层调用，实际调用了doPut方法
#### 序列化字节块写入方法putBytes
putBytes方法用于将序列化字节组成的Block写入存储系统，实际调用了doPut方法
#### 数据写入方法doPut
putSingle、putBytes等方法真正的数据写入实际由doPut实现
1. 获取putBlockInfo 如果blockInfo中已经缓存了BlockInfo，则使用缓存的BlockInfo 否则使用新建的BlockInfo
2. 获取块最终使用的存储级别putLevel，根据putLevel判断块写入的BlockStore 优先使用MemoryStore 其次是TachyonStore和DiskStore 根据data的实际包装类型，分别调用BlockStore不同的方法，putIterator putArray putBytes
3. 写入完毕后，将写入操作导致从内存drop掉的Block更新到uodateBlocks:ArrayBuffer[(BlockId,BlockStatus)]中。使用getCurrentBlockStatus获取写入Block的状态 将putBlockInfo设置为允许其他线程读取，调用reportBlockStatus将当前Block的信息更新到BlockManagerMasterActor,最后将putBlockInfo添加到updateBlocks中， updateBlocks中的Block的状态由于都发生了变化，所以都需要向BlockManagerMasterActor发送updateBlockInfo消息
4. 如果putLevel.replication大于1,即为容错考虑,数据的备份数量大于1的时候，需要将Block的数据备份到其他节点上，备份工作由replicate完成
#### 数据块备份方法replicate
为了容灾，peersForReplication中缓存的BlockManager不应当是当前的BlockManager 获取其他所有BlockManager的方法是getPeers 。。。 总结复制过程：
1. 调用getRandomPeer随机获取BlockManager
2. 上传Block到BlockManager
3. 如果上传成功，则将此BlockManager添加到peersReplicatedTo,而从peersForReplication中移除，设置replicationFailed等于false，done等于true 如果上传过程出现异常，则将此BlockManager添加到peersFailedToReplicateTo,failure自赠，设置replicationFailed等于true，done等于false

如果上传失败，以上过程会迭代多次，直到失败次数failure超过最大失败次数maxReplicationFailures
#### 创建DiskBlockObjectWriter的方法getDiskWriter
getDiskWriter用于创建DiskBlockObjectWriter 属性spark.shuffle.sync决定写操作是同步还是异步
#### 获取本地Block数据方法getBlockData
getBlockData用于从本地获取Block的数据 处理实现如下：
1. 如果Block是ShuffleMapTask的输出，那么多个partition的中间结果都写入了同一个文件，怎样读取不同partition的中间结果？IndexShuffleBlockManager的getBlockData方法解决了这个问题
2. 如果Block是ResultTask的输出，则使用doGetLocal来获取本地中间结果数据
#### 获取本地shuffle数据方法doGetLocal
当reduce任务与map任务处于同一节点时，不需要远程拉取，只需调取doGetLocal方法从本地获取中间处理结果即可 处理步骤：
1. 如果Block允许使用内存，则调用MemoryStore的getValues或者getBytes方法获取
2. 如果Block允许使用Tachyon，则调用TachyonStore的getBytes方法获取
3. 如果Block允许使用DiskStore，则调用DiskStore的getBytes方法获取
#### 获取远程Block的方法doGetRemote
doGetRemote用于从远端节点上获取Block数据 步骤：
1. 向BlockManagerMasterActor发送GetLocations消息Block数据存储的BlockManagerId。如果Block数据复制份数多于1个，则会返回多个BlockManagerId，对于这些BlockManagerId洗牌，避免总是从一个远程BlockManager获取Block数据，发送GetLocations消息使用了GetLocations
2. 根据返回的BlockManagerId信息，使用BlockTransferService远程同步获取Block数据
#### 获取Block数据方法get
get方法用于通过BlockId获取Block get方法在实现上首先从本地获取，如果没有则去远程获取
#### 数据流序列化方法dataSerializeStream
如果写入存储体系的数据本身是序列化的，那么读取时应该对其反序列化。dataSerializeStream方法使用CompressionCodec对文件输入流进行压缩和序列化处理

### metadataCleaner和broadcastCleaner
为了有效利用磁盘空间和内存，metadataCleaner和broadcastCleaner分别用于清除blockInfo中很久不用的非广播和广播Block信息  metadataCleaner与之前介绍的不同

dropOldBlocks遍历blockInfo将很久不用的Block从MemoryStore、DiskStore、TachyonStore中清除

### 缓存管理器CacheManager
CacheManager用于缓存RDD某个分区计算后的中间结果。CacheManager只是对BlockManager的代理，真正的缓存依然使用BlockManager。在人物迭代计算的过程中，当判断存储级别使用了缓存 就会调用CacheManager的getOrCompute方法 处理逻辑：
1. 从存储体系获取Block
2. 如果确实获取到了Block，那么将它封装为InterruptibleIterator并返回。如果还没有缓存Block，则重新计算或者从CheckPoint中获取数据，并调用putInBlockManager方法将数据写入缓存后封装为InterruptibleIterator并返回。

从putInBlockManager的实现，处理步骤：
1. 获取实际的存储级别
2. 如果存储级别不允许使用内存，那么直接调用BlockManager的putIterator方法。在doPut方法的处理中，由于存储级别不允许使用内存，所以数据实际被直接写入了磁盘或者Tachyon.
3. 如果存储级别允许使用内存，那么首先尝试展开。如果成功展开，说明有足够内存可以存储数据，因为将数据存入内存，如果展开失败，则将数据存入磁盘
### 压缩算法
为了节省磁盘存储空间，有些情况下需要对Block进行压缩。根据配置属性spark.io.compression.codec来确定要使用的压缩算法(默认snappy，此压缩算法在牺牲少量压缩比例的条件下，却极大地提高了压缩速度)，并生成SnappyCompressionCodeC的实例
### 磁盘写入实现DiskBlockObjectWriter
DiskBlockObjectWriter被用于输出Spark任务的中间计算结果。DiskBlockObjectWriter的fileSegment方法用于创建文件分片FileSegment(FileSegment记录分片的起始、结束偏移量)
1. 打开一个文件输出流 DiskBlockObjectWriter的open方法，利用NIO、压缩、缓存、序列化方式打开一个文件输出流
2. 写入文件 DiskBlockObjectWriter的write方法用于将数据写入文件，并更新测量信息
3. 关闭文件输出流 DiskBlockObjectWriter的close方法用于关闭文件输出流，并更新测量信息
4. 缓存数据提交 DiskBlockObjectWriter的commitAndClose方法将缓存数据写入磁盘并关闭缓存，然后更新测量数据
### 块索引shuffle管理器IndexShuffleBlockManager
IndexShuffleBlockManager通常用于获取Block索引文件，并根据索引文件读取Block文件的数据
1. 获取shuffle文件方法getBlockData：有时不知道Block的BlockId 所以无法使用BlockManager的get方法获取Block 如果知道ShuffleBlockId，依然可以通过ShuffleBlockId记录的shuffleId和mapId获取Block getBlockData方法根据shuffleId和mapId(即partitionId)读取索引文件，从索引文件中获得partition计算中间结果写入文件的偏移量和中间结果的大小，根据此偏移量和大小读取文件中partition的中间计算结果
2. 获取shuffle数据文件方法getDataFile：getDataFile的实质是调用DiskBlockManager的getFile方法
3. 索引文件偏移量记录方法writeIndexFile writeIndexFile方法用于在Block索引文件中记录各个partition的偏移量信息，便于下游Stage的任务读取
### shuffle内存管理器ShuffleMemoryManager
ShuffleMemoryManager用于为执行shuffle操作的线程分配内存池，每种磁盘溢出集合都能从这个内存池获得内存。当溢出集合的数据已经输出到存储系统，获得的内存会释放。当线程执行的任务结束，整个内存池都会被Executor释放。ShuffleMemoryManager会保证每个线程都能合理地共享内存，而不会使得一些线程获得了很大的内存，导致其他线程经常不得不将溢出的数据写入磁盘

尝试获得内存方法tryToAcquire：此方法用于当前线程尝试获得numBytes大小的内存，并返回实际获得的内存大小

ShuffleMemoryManager处理逻辑：假设当前有N个线程，必须保证每个线程在溢出之前至少获得1/2N的内存。并且每个线程最多获得1/N的内存。由于N是动态变化的变量，所以要持续对这些线程进行跟踪，以便无论何时在这些线程发生变化时重新按照1/2N和1/N计算
### 小结
开始介绍了BlockStore的接口定义，目前虽然只有MemoryStore、DiskStore、TachyonStore三种实现，但有更优秀的存储中间件出现时，随时可以实现BlockStore，完成集成。

DiskStore对磁盘文件按照散列存储节省空间的同时提高了文件访问的效率。

MemoryStore基于内存做了大量优化，其构建的内存模型值得任何大数据引擎借鉴

TachyonStore解决了Spark中共享磁盘文件系统性能差、计算引擎出错导致存储体系的数据丢失、内存中大量重复数据导致GC时间长等问题

此外，FileSegment与Block索引文件共同解决了分区数据读写同一文件的问题

shuffle服务基于Netty实现的Block上传下载服务

shuffle任务通过(1/2N,1/N)的区间管理获得内存的大小保证每个线程都能合理地共享内存并减少磁盘I/O操作

Executor的BlockManager与Driver的BlockManager上的BlockManagerMasterActor在ActorSystem中通信，保证Driver获取实时的Block状态信息
