---
title: hadoop
tags:
 - hadoop
categories: 经验分享
---
## 计算引擎
### 迭代计算
MappedRDD的iterator方法实际是父类RDD的iterator方法。如果分区任务初次执行，此时还没有缓存，所以会调用computeOrReadCheckpoint方法。

iterator容错处理过程：如果某个分区任务执行失败，但是其他分区任务执行成功，可以利用DAG重新调度。失败的分区任务将从检查点恢复状态，而那些执行成功的分区任务由于其执行结果已经缓存到存储体系，所以调用CacheManager的getOrCompute方法获取即可，不需要再次执行。

computeOrReadCheckpoint在存在检查点时直接获取中间结果，否则需要调用compute继续计算

MappedRDD的compute方法首先调用firstParent找到其父RDD 经过RDD管道中对iterator和computeOrReadCheckpoint的层层调用，最终到达HadoopRDD

HadoopRDD的compute方法用来创建NextIterator的匿名内部类，然后将其封装为InterruptibleIterator

构造NextIterator过程：
1. 从broadcast中获取jobConf，此处的jobConf 是hadoopConfiguration
2. 创建InputMetrics用于计算字节读取的测量信息，然后在RecordReader正式读取数据之前创建bytesReadCallback。bytesReadCallback用于获取当前线程从文件系统读取的字节数。
3. 获取inputFormat,此处的inputFormat为TextInputFormat
4. 使用addLocalConfiguration给JobConf添加Hadoop任务相关配置。
5. 创建RecordReader
6. 将NextIterator封装为InterruptibleIterator

整个rdd.iterator调用结束，最后返回InterruptibleIterator对象后，会调用SortShuffleWriter的write方法，功能：
1. 创建ExternalSorter，然后调用insertAll将计算结果写入缓存
2. 调用shuffleBlockManager.getDataFile方法获取当前任务要输出的文件路径
3. 调用shuffleBlockManager.consoilddateId创建BroadcastBlockId
4. 调用ExternalSorter的writePartitionedFile将中间结果持久化
5. 调用shuffleBlockManager.writeIndexFile方法创建索引文件
6. 创建MapStatus
### 什么是Shuffle
Shuffle是所有MapReduce计算框架所必须经过的阶段，shuffle用于打通map任务的输出与reduce任务的输入，map任务的中间输出结果按照key值哈希后分配给某一个reduce任务
![spark](http://jerryshao.me/img/2014-01-04-spark-shuffle/mapreduce-process.jpg)

spark早期版本shuffle
![spark](http://jerryshao.me/img/2014-01-04-spark-shuffle/spark-shuffle.png)
解释：
1. map任务会为每一个reduce任务创建一个bucket。假设有M个map任务，R个reduce任务，则map阶段一共会创建M × R个bucket
2. map任务会将产生的中间结果按照partition写入不同的bucket中
3. reduce任务从本地或者远端的map任务所在的BlockManager获取相应的bucket作为输入

Spark早期的shuffle过程存在以下问题：
1. map任务的中间结果首先存入内存，然后写入磁盘。这对于内存的开销很大，当一个节点上map任务的输出结果集很大时，很容易导致内存紧张，进而发上内存溢出
2. 每个map任务都会输出R(reduce任务数量)个bucket。假如M等于1000,R也等于1000,那么生成100万个bucket，在bucket本身不大，但是shuffle很频繁的情况下，磁盘I/O将成为性能瓶颈。
Hadoop Mapduce的shuffle过程存在以下问题：
1. reduce任务获取到map任务的中间输出后，会对这些数据在磁盘上进行merge sort，虽然不怎么占用内存，但是却产生了更多的磁盘I/O
2. 当数据量很小，但是map任务和reduce任务数目很多时，会产生很多网络I/O
为了解决以上shuffle过程中的性能问题，目前spark的shuffle已经做了多种性能优化，主要解决方法包括：
1. 将map任务给每个partition的reduce任务输出的bucket合并到同一个文件中，这解决了bucket数量很多，但是本身数据体积并不大时，造成shuffle很频繁，磁盘I/O成为性能瓶颈的问题
2. map任务逐条输出计算结果，而不是一次性输出到内存，并使用AppendOnlyMap缓存及其聚合算法对中间结果进行聚合，这大大减少了中间结果所占的内存大小
3. 对SizeTrackingAppendOnlyMap和SizeTrackingPairBuffer等缓存进行溢出判断 当超出myMemoryThreshold的大小时，将数据写入磁盘，防止内存溢出
4. reduce任务对拉取到的map任务中间结果逐条读取，而不是一次性存入内存，并在内存中进行聚合和排序(其本质上也使用了AppendOnlyMap缓存)这也大大减小了数据占用的内存
5. reduce任务将要拉取的Block按照BlockManager地址划分，然后将同一BlockManager地址中的Block累积为少量网络请求，减少网络I/O.
### map端计算结果缓存处理
* bypassMergeThreshold:传递到reduce端再做合并(merge)操作的阈值。如果partition的数量小于bypassMergeThreshold的值，则不需要在Executer执行聚合和排序操作，只需要将各个partition直接写到Executer的存储文件，最后在reduce端再做串联。通过配置spark.shuffle.sort.bypassMergeThreshold可以修改bypassMergeThreshold的默认值是200
* bypassMergeSort:标记是否传递到reduce端再做合并和排序，即是否直接将各个partition直接写到Executer的存储文件。当没有定义aggregator、ordering函数，并且partition的数量小于等于bypassMergeThreshold时，bypassMergeSort为true，map中间结果将直接输出到磁盘，此时不会占用太多内存，避免了内存撑爆问题

map端计算结果缓存有三种处理方式：
* map端对计算结果在缓存中执行聚合和排序
* map不使用缓存，也不执行聚合和排序，直接调用spillToPartitionFiles将各个partition直接写到自己的存储文件(即bypassMergeSort为true的情况)最后由reduce端对计算结果执行合并和排序
* map端对计算结果简单缓存
分析两种需要缓存的方式
#### map端计算结果缓存聚合
一个任务的分区数量通常很多，如果只是简单的将数据存储到Executer上。在执行reduce任务时会存在大量的网络I/O操作，之时网络I/O将成为系统性能的瓶颈，reduce任务读取map任务的计算结果变慢，导致其他想要分配到被这些map任务占用的节点的任务不得不等待或者降低本地化选择分配或者无法高效本地化。经过这样的恶性循环，整个集群将变得迟钝，新的任务长时间得不到执行或者执行变慢。

通过在map端对计算结果在缓存中执行聚合和排序，能够节省I/O操作，进而提升系统性能，这种情况下，必须要定义聚合器(aggregator)函数，以便于对计算结果按照partitionID和key聚合后排序。

1. AppendOnlyMap的缓存聚合算法
2. AppendOnlyMap的容量增长
3. AppendOnlyMap大小采样
#### map端计算结果简单缓存
ExternalSorter的insertAll方法中，如果没有定义aggregator，那么shouldCombine为false。这时会调用SizeTrackingPairBuffer的insert方法，从其实现可以知道，只不过是把计算结果简单的缓存到了数组中

SizeTrackingPairBuffer的容量增长是通过growArray方法实现的。growArray实现增长data数组容量的方式非常简单，只是新建2倍大小的新数组，然后简单复制而已

Spark使用SizeTrackingPairBuffer的过程中，也会调用maybeSpillCollection方法，来处理SizeTrackingPairBuffer溢出。

#### 容量限制
既然AppendOnlyMap和SizeTrackingPairBuffer的容量都可以增长，那么数据量不大的时候不会有问题。但是大数据处理数据量往往都很大，全都放入内存会将系统的内存撑爆，Spark为了防止这个问题的发生，提供了函数maybeSpillCollection
##### 集合溢出判定
maybeSpillCollection判定集合是否溢出主要由maybeSpill函数来决定，maybeSpill处理步骤：
1. 为当前线程尝试获取amountToRequest大小的内存(amountToRequest=2*currentMemory-myMemoryThreshold)。
2. 如果获得的内存依然不足(myMemoryThreshold<=currentMemory) 则调用spill，执行溢出操作。内存不足可能是申请到的内存为0或者已经申请得到的内存大小超过了myMemoryThreshold
3. 溢出后续处理，如elementsRead归零，已溢出内存字节数(memoryBytesSpilled)增加线程当前内存大小(currentMemory)，释放当前线程占用的内存
##### 溢出
如果bypassMergeSort为真，则调用spillToPartitionFiles将内存中的数据溢出到分区文件，如果bypassMergeSort不为真，则调用spillToPartitionFile

spillToPartitionFile步骤：
1. 调用createTempShuffleBlock创建临时文件
2. 新建ShuffleWriteMetrics用于测量。
3. 调用getDiskWriter方法创建DiskBlockObjectWriter
4. 调用destructiveSortedIterator方法对集合元素排序
5. 将集合内容写入临时文件。写入时机有两个: 集合遍历完的时候，执行flush  /遍历过程中，每当写入DiskBlockObjectWriter的元素个数(objectWritten)达到批量序列化尺寸(serializerBatchSize)时，也会执行flush，然后重新创建DiskBlockObjectWriter

### map端计算结果持久化
writePartitionedFile用于持久化计算结果，此方法两个分支：
* 溢出到分区文件后合并：将内存中缓存的多个partition的计算结果分别写入多个临时Block文件，然后将这些Block文件的内容全部写入正式的Block输出文件中。
* 内存中排序合并：将缓存的中间计算结果按照partition分组后写入Block输出文件，此种方式还需要更新此任务与内存、磁盘有关的测量数据

无论哪种排序方式，每个partition都会最终写入一个正式的Block文件，所以每个map任务实际上最后只会生成一个磁盘文件，最终解决了Spark早期版本中一个map任务输出的bucket文件过多和磁盘I/O成为性能瓶颈的问题。此外，无论哪种排序方式，每输出完一个partition的中间结果时，都会记录当前partition的长度，此长度将记录在索引文件中，以便下游任务的读取
#### 溢出分区文件
spillToPartitionFiles用于将内存中的集合数据按照每个partition创建一个临时Block文件，为每个临时Block文件生成DiskBlockObjectWriter，并且用DiskBlockObjectWriter将计算结果分别写入这些临时Block文件中。createTempShuffleBlock方法创建临时的Block

spillToPartitionFiles方法为每个partition生成的临时文件最后会逐个读取并统一写入正式的Block文件。 spillToPartitionFiles方法在bypassMergeSort为true，SizeTrackingAppendOnlyMap或者SizeTrackingAppendOnlyMap的大小超过myMemoryThreshold时被调用，以防止内存撑爆问题。 此外由于每个partition生成的临时文件最后会逐个读取并统一写入正式的Block文件，所以每个map任务实际上最后只会生成一个磁盘文件(相当于多个bucket被合并到一个文件中)，最终解决了产生bucket文件过多和磁盘I/O成为性能瓶颈的问题
#### 排序与分区分组
partitionedIterator通过对集合按照指定的比较器进行排序，并且按照partition id分组，生成迭代器
##### 比较器
目前三种比较器：
* keyComparator:按照指定的key进行排序
* partitionComparator:按照partition id进行比较
* partitionKeyComparator:先按照partition id进行比较，再按照指定的key进行第二级排序。当没有指定排序字段并且没有指定聚合函数时会退化为partitionComparator

partitionedIterator方法实际是通过调用destructiveSortedIterator和groupByPartition来实现
##### 排序
destructiveSortedIterator处理步骤：
1. 将data数组向左整理排序
2. 利用Sorter、KVArraySortDataFormat以及指定的比较器进行排序。其中用到了TimSort也就是优化版的归并排序
3. 生成新的迭代器
##### 分区分组
groupByPartition主要用于对destructiveSortedIterator生成的迭代器按照partition id分组
#### 分区索引文件
无论采用哪种缓存处理，在持久化的时候都会被写入同一文件，那么reduce任务如何从此文件中按照分区读取数据？

IndexShuffleBlockManager的writeIndexFile方法生成的索引文件使用偏移量来区分各个分区的计算结果，偏移量来自于合并排序过程中记录的各个partition的长度。

### reduce端读取中间计算结果
当map任务相关Stage的任务都执行完毕后，会唤起下游Stage的提交及任务的执行  上游的任务的执行结果必然是下游任务的输入，下游任务如何读取上游任务计算结果？

ResultTask的计算也是RDD的iterator方法驱动，ShuffleMapTask计算过程最终会落实到ShuffledRDD的compute方法。ShuffledRDD的compute方法首先调用SortShuffleManager的getReader方法创建HashShuffleManager，然后执行ShuffleReader的read方法读取依赖任务的中间计算结果

HashShuffleReader用来读取上游任务计算结果，他的read方法的步骤：
1. 从远端节点或者本地读取中间计算结果
2. 对InterruptibleIterator执行聚合
3. 对InterruptibleIterator排序
从远端节点或者本地读取中间计算结果通过调用BlockStoreShuffleFetcher方法实现，步骤：
1. 获取map任务执行的状态信息
2. 按照中间结果所在节点划分各个Block
3. 创建ShuffleBlockFetcherIterator(即从远端节点或者本地读取中间计算结果)
4. 将ShuffleBlockFetcherIterator封装为InterruptibleIterator

#### 获取map任务状态
Spark通过调用mapOutputTracker的getServerStatuses来获取map任务执行的状态信息，步骤：
1. 从当前BlockManager的mapOutputTracker中获取MapStatus，若没有则进入第二步，否则直接到第四步
2. 如何获取列表(fetching)中已经存在要取的shuffleId，那么就等待其他线程获取。如果获取列表中不存在要取的shuffleId，那么就将shuffleId放入获取列表
3. 调用askTracker方法向mapOutputTrackerMasterActor发送GetMapOutputStatus消息获取map任务的状态信息。mapOutputTrackerMasterActor接收到GetMapOutputStatus消息后，将请求的map任务状态信息后进行反序列化操作，然后放入本地的mapStatuses中
4. 调用mapOutputTracker的convertMapStatuses方法将获得的MapStatus转换为map任务所在的地址(BlockManagerId)和map任务输出中分配给当前reduce任务的Block大小

#### 划分本地与远程Block
无论从本地还是从mapOutputTrackerMasterActor获取的状态信息，都需要按照地址划分并且转换为BlockId。ShuffleBlockFetcherIterator是读取中间结果的关键。构造ShuffleBlockFetcherIterator初始化过程：
1. 使用splitLocalRemoteBlocks方法划分本地读取和远程读取的Block的请求
2. 将FetchRequest随机排序后存入FetchRequests:newQueue[FetchRequest]
3. 遍历FetchRequests中的所有FetchRequest，远程请求Block中间结果
4. 调用fetchLocalBlocks获取本地Block

splitLocalRemoteBlocks方法用于划分哪些Block从本地获取，哪些需要远程拉取，是获取中间计算结果的关键，定义：
* targetRequestSize:每个远程请求的最大尺寸
* totalBlocks:统计Block总数
* localBlocks:ArrayBuffer[BlockId]，缓存可以在本地获取的Block的BlockId
* remoteBlocks:HashSet[BlockId]，缓存需要远程获取的Block的BlockId
* curBlocks:ArrayBuffer[(BlockId,Long)],远程获取的累加缓存，用于保证每个远程请求的尺寸不超过targetRequestSize。 为什么要累加缓存？如果向一个机器节点频繁地请求字节数很小的Block,那么势必造成网络拥塞并增加节点负担。将多个小数据量的请求合并为一个大的请求将避免这些问题，提高系统性能
* curRequestSize:当前累加到curBlocks中的所有Block的大小之和，用于保证每个远程请求的尺寸不超过targetRequestSize
* remoteRequests:new ArrayBuffer[FetchRequest]，缓存需要远程请求的FetchRequest对象
* numBlocksToFetch:一共要获取的Block数量
* maxBytesInFlight:单次航班请求的最大字节数，什么是航班？就是一批请求，这批请求的字节总数不能超过maxBytesInFlight，而且每个请求的字节数不能超过maxBytesInFlight的五分之一。为了提高请求的并发数，允许5个请求分别从5个节点获取数据，最大限度利用各节点的资源

splitLocalRemoteBlocks处理逻辑：。。。
#### 获取远程Block
sendRequest方法用于远程请求中间结果。sendRequest利用FetchRequest里封装的blockId、size、address等信息，调用ShuffleClient的fetchBlocks方法获取其他节点上的中间计算结果
#### 获取本地Block
fetchLocalBlocks用于对本地中间计算结果的获取。利用BlockManager的getBlockData方法获取本地Block，最后将取到的中间结果存入results中

### reduce端计算
#### 如何同时处理多个map任务的中间结果
reduce任务的上游map任务可能有多个，这些中间结果的Block及数据缓存在ShuffleBlockFetcherIterator的results中。ShuffleBlockFetcherIterator作为迭代器，每次迭代，会先从results中取出一个FetchResult，并构造此FetchResult的迭代器iteratorTry，具体迭代的数据就是从iteratorTry中获取。每当iteratorTry迭代结束，才会迭代ShuffleBlockFetcherIterator
#### reduce端在缓存中对中间计算结果执行聚合和排序
reduce端获取map端任务计算中间结果后，将ShuffleBlockFetcherIterator封装为InterruptibleIterator并聚合。聚合操作主要依赖aggregator的combineCombinersByKey方法

### map端与reduce端组合分析
#### 在map端溢出分区文件，在reduce端合并组合
![spark](http://upload-images.jianshu.io/upload_images/4914401-c8b4a685a6bdd9d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
bypassMergeSort标记是否传递到reduce端再做合并和排序，此种情况不使用缓存，而是先将数据按照partition写入不同文件，最后按partition顺序合并写入同一文件。将没有指定聚合、排序函数，且partition数量较小时，一般采用这种方式。此种方式将多个bucket合并到同一个文件，通过减少map输出的文件数量，节省了磁盘I/O，最终提升了性能
#### 在map端简单缓存、排序分组，在reduce端合并组合
![spark](http://upload-images.jianshu.io/upload_images/4914401-40a7c1827ff8f526.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
此种情况在缓存中利用指定的排序函数对数据按照partition或者key进行排序，最后按partition顺序合并写入同一文件。当没有指定聚合函数，且partition数量大时，一般采用这种方式。此种方式将多个bucket合并到同一个文件，通过减少map输出的文件数量，节省了磁盘I/O，提升了性能，对SizeTrackingPairBuffer的缓存进行溢出判断，当超出myMemoryThreshold的大小时，将数据写入磁盘，防止内存溢出
#### 在map端缓存中聚合、排序分组，在reduce端组合
![spark](http://upload-images.jianshu.io/upload_images/4914401-40a7c1827ff8f526.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
此种情况在缓存中对数据按照key聚合，并且利用指定的排序函数对数据按照partition或者key进行排序，最后按partition顺序合并写入同一文件，通过减少map输出的文件数量，节省了磁盘I/O，提升了性能，对中间输出数据不是一次性读取，而是逐条放入AppendOnlyMap的缓存，并对数据进行聚合，减少了中间结果占用的内存大小，对AppendOnlyMap的缓存进行溢出判断，当超出myMemoryThreshold的大小时，将数据写入磁盘，防止内存溢出。
### 小结
* RDD迭代计算是如何实现容错的？基于缓存和检查点
* map任务是如何聚合的？使用AppendOnlyMap提供的数据结构来实现聚合
* 为什么有时候需要在map端聚合？为了降低网络I/O，提升性能
* map任务如何输出？按照分区排序或者分组后生成分区文件，并创建分区索引文件标记文件中各个分区数据的偏移量和长度
* reduce任务是如何获取map任务的输出的？通过mapOutputTracker获取到map任务所在Executor的BlockManagerId和Block的大小，然后使用ShuffleClient下载
* 发送请求时有哪些性能优化？对请求分批发送，限制分批请求的大小，并行发送请求以及将多个请求数据小的请求合并等。
